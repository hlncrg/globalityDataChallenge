{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globality Data Challenge\n",
    "\n",
    "In the following exercise, I need to classify the 'case labels' of data in a given file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "I begin by loading the data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileName='consulting101.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(fileName, 'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    data=list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first line is the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'Case Type', 'Cousulting Ferm', 'Industry Coverage', 'text', 'title']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging labels\n",
    "Many of the case type labels are repetative so I will merge them using clustring and simhash (https://en.wikipedia.org/wiki/SimHash).\n",
    "\n",
    "The clustering that I use is hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simhash import Simhash\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all of the labels if there are multiples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caseLabelAll=list(set([y for x in data[1:] \n",
    "                       for y in x[1].split(' | ')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the similarity matrix between labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity=[\n",
    "    [Simhash(x).distance(Simhash(y)) for x in caseLabelAll]\n",
    "        for y in caseLabelAll]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the labels into 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters=30\n",
    "model = AgglomerativeClustering(\n",
    "        affinity='precomputed',\n",
    "    linkage='average',\n",
    "        n_clusters=n_clusters)\n",
    "labels=model.fit_predict(connectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping between the labels and a numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caseToNum={caseLabelAll[j]: label for j,label in enumerate(labels) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math problem, brain teaser\n",
      "quantitative case, math problem\n",
      "math problem, quantitative case\n",
      "math problem\n",
      "\n",
      "finance\n",
      "finance & accounting\n",
      "\n",
      "new technology, new product\n",
      "new technology\n",
      "new product, new technology\n",
      "new product\n",
      "\n",
      "pricing and valuation\n",
      "pricing\n",
      "pricing, valuation\n",
      "pricing & valuation\n",
      "valuation\n",
      "\n",
      "private equity (PE), investment\n",
      " investment\n",
      "private equity, investment\n",
      "private equity\n",
      "private equity and investment\n",
      "investment\n",
      "private equity & investment\n",
      "private equity (PE),  investment\n",
      "investment, private equity\n",
      "\n",
      "market entry, new market\n",
      "market entry/new market\n",
      "market entry\n",
      "\n",
      "growth strategy\n",
      "marketing strategy\n",
      "strategy\n",
      "\n",
      "operations strategy/marketing\n",
      "operations strategy, supply chain optimization\n",
      "operations strategy, business process optimization\n",
      " operations strategy\n",
      "operations strategy & optimization\n",
      "operations strategy, marketing\n",
      "operations strategy, value chain optimization\n",
      "operations strategy, optimization\n",
      "operations strategy, value chain\n",
      "operations strategy, outsourcing\n",
      "business operations strategy\n",
      "operations strategy, optimization, prioritization\n",
      "marketing/operations strategy\n",
      "operations strategy\n",
      "operations strategy, product portfolio optimization\n",
      "\n",
      "economics & finance\n",
      "economics\n",
      "finance/economics\n",
      "finance, economics\n",
      "finance & economics\n",
      "\n",
      "increase sale or market share\n",
      "increase sales, revenues\n",
      "increase sale/market share\n",
      "increase sales, increase revenue\n",
      "increase sales/revenue\n",
      "increase sale/revenues\n",
      "increase sale\n",
      "increase sales and market share\n",
      "increase sale, increase market share\n",
      "increase sale/revenue\n",
      "increase market share\n",
      "increase sale & market share\n",
      "increase sales, increase market share\n",
      "increase sales/revenues/market share\n",
      "increase sales/market share\n",
      "increase sales or revenues\n",
      "increase sales\n",
      "increase sales/revenues\n",
      "\n",
      "business competition/competitive response\n",
      "business competition, competitive analysis\n",
      "business competition, competitive response\n",
      "business competition, competitive threat\n",
      "business competition/competitive benchmarking\n",
      "business competition, competitive benchmarking\n",
      "competition\n",
      "business competition, competitive benchmark\n",
      "business competition\n",
      "\n",
      "increase revenue, increase price\n",
      "increase revenues\n",
      "increase revenue\n",
      "\n",
      "reduce cost\n",
      "reduce costs, outsourcing\n",
      "reduce costs\n",
      "\n",
      "growth, business expansion\n",
      "growth, expansion\n",
      "growth\n",
      "growth, expand capacity\n",
      "\n",
      "estimate or guesstimate\n",
      "estimation, guesstimate\n",
      "estimate\n",
      "estimate, guesstimate \n",
      "estimate/guesstimate\n",
      "estimate, guesstimate\n",
      "\n",
      "optimization\n",
      "supply chain optimization\n",
      "business optimization\n",
      "\n",
      "improve profits\n",
      "improve profit\n",
      "improve profits/profitability\n",
      "improve profitability\n",
      "\n",
      "business turnaround\n",
      "\n",
      "HR, organizational behavior\n",
      "human capital, organizational behavior\n",
      "organizational behavior, supply chain optimization\n",
      "HR/organizational behavior\n",
      "human capital, HR, organizational behavior\n",
      "human resources, organizational behavior\n",
      "organizational behavior, HR\n",
      "organizational behavior\n",
      "organizational behavior, human capital\n",
      "\n",
      "new business\n",
      "new business, new technology\n",
      "new business/technology\n",
      "\n",
      "\n",
      "\n",
      "increase profit\n",
      "\n",
      "new business, diversification\n",
      "\n",
      "business expansion & growth\n",
      "business expansion, growth\n",
      "\n",
      "industry analysis, marketing\n",
      "industry analysis\n",
      "\n",
      "market sizing\n",
      "\n",
      "competitive response\n",
      "\n",
      "improve profits/bottom line\n",
      "improve profit/bottom line\n",
      "improve profitability/bottom line\n",
      "\n",
      "business expansion, add capacity\n",
      "add capacity & growth\n",
      "adding capacity, expansion\n",
      "add capacity, expansion, growth\n",
      "add capacity, business expansion, growth\n",
      "add capacity, business expansion\n",
      "add capacity, expansion\n",
      "add capacity, growth\n",
      "growth, add capacity\n",
      "add capacity\n",
      "\n",
      "mergers and acquisitions (M&A)\n",
      "mergers & acquisitions\n",
      "merger and acquisition\n",
      "mergers and acquisitions\n",
      "mergers & acquisitions (M&A)\n",
      "merger and acquisition (M&A)\n",
      "merger & acquisition\n",
      "merger & acquisition (M&A)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(30):\n",
    "    for i,label in enumerate(labels):\n",
    "        if label==x:\n",
    "            print caseLabelAll[i]\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge companies\n",
    "\n",
    "Similar to the label data, we need to consolidate the labels of the companies since they are often named different things but are the same company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CousultingFermAll=list(set([y for x in data[1:] for y in x[2].split(' | ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connectivity=[\n",
    "    [Simhash(x).distance(Simhash(y)) for x in CousultingFermAll]\n",
    "        for y in CousultingFermAll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters=110\n",
    "model = AgglomerativeClustering(\n",
    "        affinity='precomputed',\n",
    "    linkage='average',\n",
    "        n_clusters=n_clusters)\n",
    "labels=model.fit_predict(connectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firmToNum={CousultingFermAll[j]: label for j,label in \n",
    "           enumerate(labels) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in range(125):\n",
    "    for i,label in enumerate(labels):\n",
    "        if label==x:\n",
    "            print CousultingFermAll[i]\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industry coverage\n",
    "\n",
    "Similarly for the industry, we consolidate the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indLabelAll=list(set([y for x in data[1:] \n",
    "                      for y in x[3].split(' | ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connectivity=[\n",
    "    [Simhash(x).distance(Simhash(y)) for x in indLabelAll]\n",
    "        for y in indLabelAll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters=100\n",
    "model = AgglomerativeClustering(\n",
    "        affinity='precomputed',\n",
    "    linkage='average',\n",
    "        n_clusters=n_clusters)\n",
    "labels=model.fit_predict(connectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in range(125):\n",
    "    for i,label in enumerate(labels):\n",
    "        if label==x:\n",
    "            print indLabelAll[i]\n",
    "    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indToNum={indLabelAll[j]: label for j,label in enumerate(labels) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text\n",
    "\n",
    "To process the text, I first count the word usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "[stopWords.add(w) for w in ['!','#','$','%','&','(',')',',','.']];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stem the word and remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allWords=[w2 for w2 in \n",
    " [stemmer.stem(w) \n",
    "    for x in data[1:]\n",
    "     for w in word_tokenize(x[4].decode('utf-8'))] \n",
    "          if w2 not in stopWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the word usage and remove unfrequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordCount={}\n",
    "for word in allWords:\n",
    "    wordCount[word]=wordCount.get(word,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocDic={w:i for i,w in enumerate([x[0] for x in wordCount.items() if x[1]>10])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1848"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocDic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature vect\n",
    "\n",
    "I create one-hot encoding vectors to encode the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encFirm = OneHotEncoder()\n",
    "encFirm.fit([[i] for i in range(len(firmToNum))])\n",
    "encInd = OneHotEncoder()\n",
    "encInd.fit([[i] for i in range(len(indToNum))])\n",
    "encVoc = OneHotEncoder()\n",
    "encVoc.fit([[i] for i in range(len(vocDic))])\n",
    "encCase = OneHotEncoder()\n",
    "encCase.fit([[i] for i in range(len(caseToNum))]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the data into training set (Xdata,Ydata) and testing set (xxData,yyData) in a ratio of 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata=[]\n",
    "Ydata=[]\n",
    "xxData=[]\n",
    "yyData=[]\n",
    "for line in data[1:]:\n",
    "    if all([l!='' for l in line]): \n",
    "        r=randint(1, 4)\n",
    "        if r!=1:\n",
    "            for lab in line[1].split(' | '):\n",
    "                vecFirm=np.sum(\n",
    "                    encFirm.transform(\n",
    "                        [[firmToNum[y]] for y in line[2].split(' | ')]).toarray(),axis=0).tolist()\n",
    "\n",
    "                vecInd=np.sum(\n",
    "                    encInd.transform(\n",
    "                        [[indToNum[y]] for y in line[3].split(' | ')]).toarray(),axis=0).tolist()\n",
    "\n",
    "                vecText=np.sum(encVoc.transform([[y] for y in list(set([vocDic[stemmer.stem(w)] \n",
    "                            for w in word_tokenize(line[4].decode('utf-8')) \n",
    "                     if stemmer.stem(w) in vocDic]))]).toarray(),axis=0).tolist()\n",
    "\n",
    "                vecTitle=np.sum(encVoc.transform([[y] for y in list(set([vocDic[stemmer.stem(w)] \n",
    "                            for w in word_tokenize(line[5].decode('utf-8')) \n",
    "                     if stemmer.stem(w) in vocDic]))]).toarray(),axis=0).tolist()\n",
    "\n",
    "                Xdata.append(vecFirm+vecInd+vecText+vecTitle)\n",
    "                Ydata.append(caseToNum[lab])\n",
    "        if r==1:\n",
    "            xxData.append(vecFirm+vecInd+vecText+vecTitle)\n",
    "            yyData.append([caseToNum[lab] for lab in line[1].split(' | ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the data\n",
    "\n",
    "I use some simple models to predict the labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use cross entropy to compare training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossEnt(x1,x2):\n",
    "    return -sum([x1i*math.log(x2i) for x1i,x2i in zip(x1,x2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I use a one-to-many logistic regression to make the predictions.  I calculate for different regularization parameters for L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.97280671539\n",
      "2.8954155611\n",
      "3.43111834398\n",
      "4.87573746106\n"
     ]
    }
   ],
   "source": [
    "for C in [0.001,0.01,0.1,1]:\n",
    "    logistic = linear_model.LogisticRegression(C=C)\n",
    "    logistic.fit(Xdata, Ydata)\n",
    "    ypreds=logistic.predict_proba(xxData)\n",
    "    print np.mean([crossEnt([1.0/len(yd) if y in yd else 0 for y in range(30)],ypred)\n",
    "    for ypred,yd in zip(ypreds,yyData)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I use a softmax logistic regression to make the predictions.  I also use different regularization parameters for L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.90441085009\n",
      "2.91450462128\n",
      "3.58556891096\n",
      "5.11802789023\n"
     ]
    }
   ],
   "source": [
    "for C in [0.001,0.01,0.1,1]:\n",
    "    logistic = linear_model.LogisticRegression(C=C,multi_class='multinomial',solver='newton-cg')\n",
    "    logistic.fit(Xdata, Ydata)\n",
    "    ypreds=logistic.predict_proba(xxData)\n",
    "    print np.mean([crossEnt([1.0/len(yd) if y in yd else 0 for y in range(30)],ypred)\n",
    "    for ypred,yd in zip(ypreds,yyData)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I use a multinomial naive bayes model with different smoothing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.1728369428\n",
      "5.01252423929\n",
      "3.12039078177\n",
      "2.92447850754\n"
     ]
    }
   ],
   "source": [
    "for alpha in [1,200.0,1000.0,10000.0]:\n",
    "    model=MultinomialNB(alpha=alpha)\n",
    "    model.fit(Xdata,Ydata)\n",
    "    ypreds=model.predict_proba(xxData)\n",
    "    print np.mean([crossEnt([1.0/len(yd) if y in yd else 0 for y in range(30)],ypred)\n",
    "        for ypred,yd in zip(ypreds,yyData)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can not that the value of cross entropy of randomly choosing a label would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4011973816621555"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-math.log(1/30.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the models do seem to be capturing some information.  But probably not as well as one would hope.  \n",
    "\n",
    "## Future work\n",
    "\n",
    "Here I used a simple 1-gram one-hot encoding model for the text data.  I could have use other models such a 2-gram, TFIDF, glove, or w2v.  But for the sake of time just went with the simpler model.  Exploring other embeddings would definitely be a good idea.  \n",
    "\n",
    "Additionally, I did not at all attempt to do any feature engineering.  I could have done this to improve the predictions.  I could have also used a neural network which does automatic feature selection.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Questions\n",
    "\n",
    "### What is part of speech (POS) tagging? What is the simplest approach to building a POS tagger that you can imagine?\n",
    "\n",
    "A part of speech tagger finds the part of speech of individual words in a sentence.  The simplest part of speech would simply assign the most common part of speech to a given term, ignore all other information from the surrounding words.\n",
    "\n",
    "### How would you build a POS tagger from scratch given a corpus of annotated sentences? How would you deal with unknown words?\n",
    "\n",
    "I would use a linear chain conditional random field model since this is one of the best yet simplest part of speech taggers and would be easily trained.  Unknown words can still be labeled using a linear chain conditional random field model because there is information about the words that come before and after the given word.\n",
    "\n",
    "### How would you train a model that identifies whether the word “Apple” in a sentence belongs to the fruit or the company?\n",
    "\n",
    "I would train a W2V model on a large corpus such a Wikipedia.  I would then calculate the W2V for the article apple as a fruit and apple as the company.  I would then calculate the cosine distance between the concept W2v and the W2V of the document that I am trying to disambiguate. \n",
    "\n",
    "### How would you find all the occurrences of quoted text in a news article?\n",
    "\n",
    "First, I would tokenize the sentene.  I would scan the sentence for a beginning quote and then for an ending quote.  If I find both then it is likely a quote.\n",
    "\n",
    "### How would you build a system that auto corrects text that has been generated by a speech recognition system?\n",
    "\n",
    "We could compare the word to similar words that are with a dictionary.  If the words are similar to one another and one is more commonly used that the other, then we might make the correction.  A more sophisticated system might also compare other words in the sentence and calculate the likelihood of that word given the other words in the sentence.\n",
    "\n",
    "### What is latent semantic indexing and where can it be applied?\n",
    "\n",
    "LSI transforms documents and terms into 'concepts' via an SVD decompostion.  Documents in a corpus are mathematically represented in a document vs. term matrix and this matrix is then decomposed using SVD.  It is useful for understanding similarities between terms and documents in terms of co-ocurrences of \n",
    "\n",
    "### How would you build a system that automatically groups news articles by subject?\n",
    "\n",
    "I would first train a LDA topic model.  I would then classify the new articles according to this model.\n",
    "\n",
    "### What are stop words? Describe an application in which stop words should be removed.\n",
    "\n",
    "Stop words are very common words that help to make the grammar of a sentence correct but are often to common to be useful for common NLP tasks and thus are often removed before processing.  The words should be removed when they contain no signal that would help in the task at hand and would just create larger feature vectors and add noise.\n",
    "\n",
    "### What is entropy? How would you estimate the entropy of the English language?\n",
    "\n",
    "Entropy is the amount of information in data.  If we have an estimate of the probability of words in the English language we can calculate entropy using the typical formula.  According to wikipedia, English text has between 0.6 and 1.3 bits of entropy per character of the message.\n",
    "\n",
    "### What is the TF-IDF score of a word and in what context is this useful?\n",
    "\n",
    "TF-IDF stands for term frequency, inverse document frequency and calculates just that --- the frequency of a term times the inverse document frequency (often with slight modification).  This is useful as a vectorization of words as features as it gives greater weight to less common words.  It is also useful for understand how descriptive a given word is.  Low score indicate a common word while high scores indicate a word with a very specific meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
